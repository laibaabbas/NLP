{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlJ5gQb2AAEohjAsDJknit",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laibaabbas/NLP/blob/main/Fake_new_Classification(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake News Detection on Social Media using Deep Learning\n",
        "\n",
        "## Classwork NLP Project (Google Colab Notebook)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "### Project Title\n",
        "\n",
        "**Fake News Detection on Social Media using Deep Learning**\n",
        "\n",
        "### Domain\n",
        "\n",
        "Social Media • Media Literacy • Security\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "Fake news on social media platforms can mislead users, influence public opinion, and pose serious social and security risks. The objective of this project is to build **deep learning–based Natural Language Processing (NLP) models** that can automatically classify news articles as:\n",
        "\n",
        "* **Real News (0)**\n",
        "* **Fake News (1)**\n",
        "\n",
        "This is a **binary text classification problem**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dataset Description\n",
        "\n",
        "### Dataset Name\n",
        "\n",
        "**WELFake Dataset**\n",
        "\n",
        "### Dataset Characteristics\n",
        "\n",
        "* Total samples: **72,134 news articles**\n",
        "* Balanced dataset\n",
        "* Textual data only\n",
        "* Two main text fields:\n",
        "\n",
        "  * `title`\n",
        "  * `text` (news content)\n",
        "* Target label:\n",
        "\n",
        "  * `0` → Real News\n",
        "  * `1` → Fake News\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Learning Objectives\n",
        "\n",
        "By completing this notebook, students will learn:\n",
        "\n",
        "* Text preprocessing for NLP tasks\n",
        "* Tokenization and sequence padding\n",
        "* Word embeddings for deep learning\n",
        "* Building multiple deep learning models for text\n",
        "* Evaluating and comparing NLP models\n",
        "* Performing predictions on unseen text data\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Environment Setup\n",
        "\n",
        "### Step 1: Import Required Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "_gvDVaIYyCs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, GRU, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OhMoLznyilno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Load the Dataset\n",
        "\n",
        "### Step 2: Upload Dataset in Colab\n",
        "\n"
      ],
      "metadata": {
        "id": "gqkM7eb9ycEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "NorXI5sNyQ_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"saurabhshahane/fake-news-classification\")"
      ],
      "metadata": {
        "id": "9kAyjJO10T-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Read the Dataset"
      ],
      "metadata": {
        "id": "Onqk6jIKy75f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "id": "9_1KwYCD1eJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = os.path.join(path, \"WELFake_Dataset.csv\")\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "2cebU8lR1B9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('WELFake_Dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Zs6WAxtey_OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 6. Data Understanding\n"
      ],
      "metadata": {
        "id": "QBSdfepW1qsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Dataset Shape:', df.shape)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "DOkjfQZc1xAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Class Distribution\n"
      ],
      "metadata": {
        "id": "Z6t3vEp-1zMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts().plot(kind='bar')\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4JiPAfJ217-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 7. Text Preprocessing\n",
        "\n",
        "### Step 4: Combine Title and Text"
      ],
      "metadata": {
        "id": "nNQjBHdu1--D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['title'] + ' ' + df['text']\n",
        "df = df[['content', 'label']]"
      ],
      "metadata": {
        "id": "AbD1EClq2FOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 5: Text Cleaning Function\n"
      ],
      "metadata": {
        "id": "rILhQaPq2GnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>+', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "df['content'] = df['content'].fillna('')  # Fill NaN values with empty strings\n",
        "df['content'] = df['content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "emlQmpyv2Owp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 8. Train-Test Split\n"
      ],
      "metadata": {
        "id": "8p3Md7CM2Ixq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['content']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "Lmzf4drM2yA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 9. Tokenization and Padding\n"
      ],
      "metadata": {
        "id": "Y3Hhw6gU2xuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50000\n",
        "max_length = 300\n",
        "oov_token = '<OOV>'\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "KSdOOrP128uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 10. Model 1: Embedding + LSTM"
      ],
      "metadata": {
        "id": "gCdEwfs93BAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=max_length),\n",
        "    LSTM(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "V5T1_EfI3FGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_lstm = model_lstm.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    callbacks=[EarlyStopping(patience=2, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC3haisK5dUx",
        "outputId": "a415e8c8-35a3-4bcf-e0b2-02b8dbce483e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 760ms/step - accuracy: 0.7335 - loss: 0.4921 - val_accuracy: 0.8025 - val_loss: 0.3622\n",
            "Epoch 2/5\n",
            "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 770ms/step - accuracy: 0.7838 - loss: 0.3853 - val_accuracy: 0.8966 - val_loss: 0.2692\n",
            "Epoch 3/5\n",
            "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 766ms/step - accuracy: 0.9242 - loss: 0.2140 - val_accuracy: 0.7251 - val_loss: 0.4253\n",
            "Epoch 4/5\n",
            "\u001b[1m722/722\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 759ms/step - accuracy: 0.8661 - loss: 0.2823 - val_accuracy: 0.9321 - val_loss: 0.1779\n",
            "Epoch 5/5\n",
            "\u001b[1m406/722\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3:46\u001b[0m 717ms/step - accuracy: 0.9675 - loss: 0.1043"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 11. Model 2: Embedding + BiLSTM\n"
      ],
      "metadata": {
        "id": "dHYMBhjV5gdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_bilstm = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=max_length),\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_bilstm.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_bilstm.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "WnNrHXz25kmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 12. Model 3: CNN for Text Classification\n"
      ],
      "metadata": {
        "id": "JC3EiS8eH-SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=max_length),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_cnn.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "3qjhHTyiICwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 13. Model 4: GRU\n"
      ],
      "metadata": {
        "id": "qubBSHi7ICO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=max_length),\n",
        "    GRU(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_gru.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_gru.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "tt2ypptiIJnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 14. Model Evaluation"
      ],
      "metadata": {
        "id": "JAiWeENpITpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
        "    plt.show()\n",
        "\n",
        "print('LSTM Evaluation')\n",
        "evaluate_model(model_lstm, X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "W93L06RjIV4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 15. Prediction on New News Article\n"
      ],
      "metadata": {
        "id": "5flPpT-cIYXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Breaking: Scientists confirm water found on Mars\"\n",
        "\n",
        "sample_seq = tokenizer.texts_to_sequences([sample_text])\n",
        "sample_pad = pad_sequences(sample_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "prediction = model_bilstm.predict(sample_pad)\n",
        "\n",
        "if prediction > 0.5:\n",
        "    print('Prediction: Fake News')\n",
        "else:\n",
        "    print('Prediction: Real News')"
      ],
      "metadata": {
        "id": "yyEtW1uyIfCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 16. Conclusion\n",
        "\n",
        "In this classwork project, multiple deep learning models were implemented for fake news detection. Students observed how different architectures (LSTM, BiLSTM, CNN, GRU) handle textual data and compared their performance. This notebook demonstrates a complete NLP pipeline suitable for real-world social media security applications.\n",
        "\n",
        "---\n",
        "\n",
        "## End of Notebook\n"
      ],
      "metadata": {
        "id": "qWFH03emy3Rw"
      }
    }
  ]
}