{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laibaabbas/NLP/blob/main/NLP_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb90caa9",
      "metadata": {
        "id": "eb90caa9"
      },
      "source": [
        "\n",
        "# üß† Natural Language Processing (NLP)\n",
        "\n",
        "## 1. Introduction to NLP\n",
        "Natural Language Processing (NLP) is a subfield of AI that enables computers to understand, interpret, and generate human language.\n",
        "\n",
        "**Applications:** Chatbots, sentiment analysis, translation, summarization, question answering, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ebc091",
      "metadata": {
        "id": "d4ebc091"
      },
      "source": [
        "\n",
        "### Why NLP?\n",
        "- Enables machines to communicate with humans in natural language\n",
        "\n",
        "- Helps extract information from text data (emails, tweets, reviews, etc.)\n",
        "\n",
        "- Powers many AI systems: chatbots, translators, summarizers, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Examples of NLP Applications\n",
        "\n",
        "| Application           | Example                             |\n",
        "| --------------------- | ----------------------------------- |\n",
        "| Sentiment Analysis    | ‚ÄúThis product is great!‚Äù ‚Üí Positive |\n",
        "| Machine Translation   | English ‚Üí French                    |\n",
        "| Text Summarization    | Condensing long articles            |\n",
        "| Chatbots              | Virtual assistants like Siri, Alexa |\n",
        "| Information Retrieval | Search engines                      |\n",
        "| Spam Detection        | Filtering junk emails               |\n"
      ],
      "metadata": {
        "id": "uUrVt4j0scws"
      },
      "id": "uUrVt4j0scws"
    },
    {
      "cell_type": "markdown",
      "id": "d678b366",
      "metadata": {
        "id": "d678b366"
      },
      "source": [
        "\n",
        "## 2. Key NLP Tasks\n",
        "- Tokenization  \n",
        "- Stopword Removal  \n",
        "- Stemming and Lemmatization  \n",
        "- POS Tagging  \n",
        "- Named Entity Recognition (NER)  \n",
        "- Bag of Words (BoW)  \n",
        "- TF-IDF (Term Frequency‚ÄìInverse Document Frequency)  \n",
        "- Word Embeddings (Word2Vec, GloVe, FastText)  \n",
        "- Transformers (BERT, GPT, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Text Data and Corpus\n",
        "\n",
        "A corpus is a large collection of text used for analysis.\n",
        "Example: news articles, tweets, Wikipedia dumps."
      ],
      "metadata": {
        "id": "LPcibzKZsqQR"
      },
      "id": "LPcibzKZsqQR"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing enables machines to understand human language.\"\n"
      ],
      "metadata": {
        "id": "LtGJ5r5GswLv"
      },
      "id": "LtGJ5r5GswLv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Tokenization\n",
        "\n",
        "Splitting text into smaller pieces (tokens)."
      ],
      "metadata": {
        "id": "oGqtdtV-symU"
      },
      "id": "oGqtdtV-symU"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to fix the LookupError\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"NLP is amazing. It helps computers understand text.\"\n",
        "print(word_tokenize(text))\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "id": "MWQk_-nUs20x"
      },
      "id": "MWQk_-nUs20x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Stopwords Removal\n",
        "\n",
        "Removing common words that don‚Äôt carry much meaning."
      ],
      "metadata": {
        "id": "vYK6s8ectOnD"
      },
      "id": "vYK6s8ectOnD"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "words = word_tokenize(\"NLP helps machines understand human language.\")\n",
        "filtered = [w for w in words if w.lower() not in stopwords.words('english')]\n",
        "print(filtered)\n"
      ],
      "metadata": {
        "id": "wOzeZ8VctWvh"
      },
      "id": "wOzeZ8VctWvh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.4 Stemming and Lemmatization\n",
        "\n",
        "- **Stemming**: Reduces words to root form (crude)\n",
        "\n",
        "- **Lemmatization**: Converts words to base form using vocabulary"
      ],
      "metadata": {
        "id": "uzN4P6QItdEo"
      },
      "id": "uzN4P6QItdEo"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "print(ps.stem(\"running\"))       # run\n",
        "print(lm.lemmatize(\"running\"))"
      ],
      "metadata": {
        "id": "vBh7yE8jtmcr"
      },
      "id": "vBh7yE8jtmcr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dfa7eda2",
      "metadata": {
        "id": "dfa7eda2"
      },
      "source": [
        "### Basic Preprocessing in NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b53b8ab",
      "metadata": {
        "id": "4b53b8ab"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"Natural Language Processing allows computers to understand human language.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(w) for w in filtered_tokens]\n",
        "print(\"Stems:\", stems)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "print(\"Lemmas:\", lemmas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Part-of-Speech (POS) Tagging\n",
        "\n",
        "Assigning grammatical labels (noun, verb, etc.) to words."
      ],
      "metadata": {
        "id": "-oCA5RKwuF2F"
      },
      "id": "-oCA5RKwuF2F"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Added to fix the LookupError"
      ],
      "metadata": {
        "id": "AO_pocuauRhJ"
      },
      "id": "AO_pocuauRhJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(\"John loves coding in Python.\")\n",
        "print(nltk.pos_tag(tokens))"
      ],
      "metadata": {
        "id": "aNm7K4Ecup0t"
      },
      "id": "aNm7K4Ecup0t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Named Entity Recognition (NER)\n",
        "\n",
        "Identifying entities like names, locations, dates, etc."
      ],
      "metadata": {
        "id": "yGmhYi8lu02h"
      },
      "id": "yGmhYi8lu02h"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Elon Musk founded SpaceX in 2002 in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "id": "gIXuvSQ2u4TX"
      },
      "id": "gIXuvSQ2u4TX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cc965933",
      "metadata": {
        "id": "cc965933"
      },
      "source": [
        "## 4. Bag of Words and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5cb70f",
      "metadata": {
        "id": "1e5cb70f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"I love NLP and machine learning.\",\n",
        "    \"NLP is amazing for text analysis.\",\n",
        "    \"Machine learning and NLP are related fields.\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "cv = CountVectorizer()\n",
        "bow = cv.fit_transform(corpus)\n",
        "print(\"Vocabulary:\", cv.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow.toarray())\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(corpus)\n",
        "print(\"TF-IDF Features:\", tfidf.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "488cb9f0",
      "metadata": {
        "id": "488cb9f0"
      },
      "source": [
        "## 5. Word Embeddings (Word2Vec Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d023671",
      "metadata": {
        "id": "6d023671"
      },
      "outputs": [],
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [\n",
        "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
        "    [\"Word2Vec\", \"creates\", \"vector\", \"representations\", \"of\", \"words\"]\n",
        "]\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
        "print(\"Vector for 'language':\\n\", model.wv['language'])\n",
        "print(\"Most similar words to 'love':\", model.wv.most_similar('love'))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f8d5af4"
      },
      "source": [
        "!pip install gensim"
      ],
      "id": "7f8d5af4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "806e0ac3",
      "metadata": {
        "id": "806e0ac3"
      },
      "source": [
        "## 6. Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983c0f75",
      "metadata": {
        "id": "983c0f75"
      },
      "outputs": [],
      "source": [
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ec7e01",
      "metadata": {
        "id": "e6ec7e01"
      },
      "source": [
        "## 7. Text Classification (Example: Sentiment Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e398c4",
      "metadata": {
        "id": "92e398c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = [\"I love this movie\", \"I hate this movie\", \"Amazing performance\", \"Terrible direction\"]\n",
        "y = [1, 0, 1, 0]\n",
        "\n",
        "vec = CountVectorizer()\n",
        "X_vec = vec.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.5)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaae29df",
      "metadata": {
        "id": "eaae29df"
      },
      "source": [
        "## 8. Transformer Model (BERT) Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a694d31",
      "metadata": {
        "id": "3a694d31"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"I really enjoy learning NLP with deep learning!\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QoH48YSpgpu9"
      },
      "id": "QoH48YSpgpu9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}